# 🛡️ Quantum Studio AI 모델 전략 가이드 (2026)

본 문서는 Quantum Studio의 핵심 가치인 **경량화, 비용 절감, 정확성, 그리고 데이터 보안**을 달성하기 위한 AI 모델 운영 전략을 정의합니다.

## 1. 모델 채택 원칙
1. **보안 우선**: 데이터 유출 우려가 있는 특정 국가 모델을 배제하고, 글로벌 표준 및 신뢰할 수 있는 오픈소스 모델을 채택합니다.
2. **비용 효율**: 상용 API(Gemini 등) 사용을 최소화하고, 로컬 리소스를 활용하는 무료 오픈소스 모델(Open Weights)을 지향합니다.
3. **경량화**: 로컬 환경(CPU/GPU)에서 원활하게 구동 가능한 소형 언어 모델(SLM)을 우선순위에 둡니다.

## 2. 추천 모델 라인업

| 계층 | 모델명 | 제조사 | 라이선스 | 주요 역할 |
|:---:|:---|:---:|:---:|:---|
| **L1: 필터** | **TinyLlama 1.1B** | 오픈소스 | MIT | 초고속 카테고리 분류 및 1차 텍스트 필터링 |
| **L2: 추론** | **Llama 3.2 (3B)** | **Meta** | **Llama 3.2 (무료*)** | 데이터 간 상관관계 분석, 논리적 추론, 요약 |
| **L3: 구조화** | **Gemini Flash/Pro** | **Google** | **API (유료/무료)** | 초거대 데이터 및 실시간 외부 정보 연동 |

> *Llama 라이선스는 월간 활성 사용자 7억 명 미만 기업에게 무료입니다.

## 3. 모델별 상세 분석 및 채택 사유

### 🟢 Llama 3.2 (3B) - [현재 메인 추론 모델]
- **채택 사유**: **속도 및 리소스 최적화**. M1 Mac 등 통합 메모리 환경에서 가장 쾌적한 응답 속도(Latency)를 제공합니다.
- **특징**: 2.0GB의 저용량으로 시스템 부하를 최소화하면서도, 한국어 이해도와 논리적 추론 능력이 검증된 글로벌 표준 모델입니다.
- **용도**: 일반적인 데이터 관계 분석, 3D 매핑 구조 생성, 실시간 대화형 비서.

### 🔵 Phi-4 (14B) - [사양 개선 시 도입 예정]
- **도입 사유**: **최고 수준의 정확성 및 전문 추론**. 데이터의 복잡도가 매우 높거나 법률/금융 등 고도의 정밀 분석이 필요할 때 사용합니다.
- **특징**: 약 9.1GB의 대용량 모델로, 소형 모델이 놓칠 수 있는 미세한 논리적 연결 고리를 찾아내는 데 탁월합니다.
- **전제 조건**: 최소 16GB 이상의 통합 메모리(RAM) 또는 고성능 외장 GPU 환경 권장.
- **향후 계획**: 하드웨어 사양 업그레이드 또는 서버급 인프라 구축 시, 정밀도 극대화를 위해 메인 엔진을 Phi-4로 전환할 예정입니다.

## 4. 운영 아키텍처 (Hybrid Strategy)

1. **Pre-processing (Local)**:
   - `TinyLlama`가 입력 데이터의 카테고리를 판별합니다.
2. **Deep Analysis (Local)**:
   - 카테고리가 결정되면 `Llama 3.2`가 로컬 VRAM에 로드되어 상세 분석 및 관계 추출을 수행합니다. (속도 우선)
   - 사양 충족 시 `Phi-4`를 호출하여 분석 정밀도를 높입니다. (정확성 우선)
3. **Structured Output (Local/Cloud)**:
   - 분석된 내용을 바탕으로 `Llama 3.2` 또는 `Gemini Flash`가 최종 시각화 JSON을 생성합니다.

## 5. 로컬 환경 설정 가이드 (Ollama 설치 및 실행)

Quantum Studio의 로컬 AI 기능을 활성화하기 위해 아래 단계를 수행하십시오.

### 1단계: Ollama 설치
1. **공식 사이트 접속**: [ollama.com/download](https://ollama.com/download)
2. **OS 선택**: macOS (또는 해당 OS) 버전을 다운로드합니다.
3. **설치**: 다운로드된 앱을 `응용 프로그램(Applications)` 폴더로 이동시킨 후 실행합니다.
4. **CLI 도구 승인**: 처음 실행 시 터미널 명령어 사용을 위한 승인 창이 뜨면 `Install`을 클릭합니다.

### 2단계: 필수 모델 다운로드
터미널(Terminal.app 또는 iTerm2)을 열고 아래 명령어를 순서대로 입력하여 모델을 내려받습니다.

```bash
# 1. 최전방 필터 모델 (TinyLlama)
ollama pull tinyllama

# 2. 메인 추론 모델 (Llama 3.2) - 약 2.0GB (M1 최적화)
ollama pull llama3.2
```

### 3단계: 실행 및 확인
모델이 정상적으로 설치되었는지 확인하려면 아래 명령어를 입력합니다.

```bash
# 설치된 모델 목록 확인
ollama list

# Llama 3.2 모델과 직접 대화 테스트
ollama run llama3.2
```

### 4단계: 모델 최적화 (경량화 및 속도 향상)
M1 Mac 환경에서 쾌적한 속도를 위해 Llama 3.2 사용을 강력히 권장합니다.

```bash
# 기존 무거운 모델 제거 (필요 시)
ollama rm phi4
```

> **참고**: `q4_K_M` 버전은 지능을 거의 유지하면서 용량을 약 2.3GB로 줄여 속도를 3~5배 향상시킵니다. 만약 고도의 정밀한 분석이 다시 필요하다면 `ollama run phi4` 명령어로 기본 모델(약 9GB)을 다시 설치할 수 있습니다.

## 6. 백엔드 연동
Ollama 앱이 실행 중인 상태(메뉴바에 아이콘 표시됨)에서 Quantum Studio 파이썬 백엔드를 구동하면, `http://localhost:11434`를 통해 자동으로 AI 분석이 시작됩니다.

## 6. 향후 과제
- **도메인 최적화**: 금융, 인프라 등 특정 카테고리에 대해 Phi-4를 LoRA 기법으로 미세 조정(Fine-tuning)합니다.
- **자동 설치 스크립트**: 프로젝트 초기화 시 Ollama 및 모델 존재 여부를 체크하고 자동 설치하는 스크립트를 구현할 예정입니다.
