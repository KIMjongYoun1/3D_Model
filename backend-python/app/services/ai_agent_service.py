"""
AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ (ê³ ë„í™” ë²„ì „)
- ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ í‹°ì–´ë§ (Local, Flash, Pro)
- ì§€ì‹ ë² ì´ìŠ¤(RAG) ì—°ë™
- í•œêµ­ì–´ ì£¼ì„ ë° ì„¤ëª… í¬í•¨
"""
import time
import json
import torch
import re
import httpx
from typing import Dict, Any, List, Optional
from google import genai
from transformers import pipeline
from sqlalchemy.orm import Session

from app.core.config import settings
from app.core.document_processor import document_processor
from app.core.categories import detect_category, CATEGORIES, ModelTier
from app.models.knowledge import KnowledgeBase

class AIAgentService:
    def __init__(self):
        self.cloud_available = False
        try:
            if settings.gemini_api_key:
                self.client = genai.Client(api_key=settings.gemini_api_key)
                self.cloud_available = True
            else:
                print("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Gemini ì´ˆê¸°í™” ì—ëŸ¬: {e}")
        
        self.local_pipeline = None
        self.local_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        self.ollama_base_url = "http://localhost:11434"
        self.request_history = []

    async def _call_ollama(self, model: str, prompt: str) -> Optional[Dict[str, Any]]:
        """Ollama APIë¥¼ í†µí•œ ë¡œì»¬ ëª¨ë¸(Phi-4 ë“±) í˜¸ì¶œ"""
        print(f"ğŸ¦™ [Mode: Ollama] Requesting {model}...")
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.ollama_base_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.2,
                            "top_p": 0.9
                        }
                    }
                )
                if response.status_code == 200:
                    response_text = response.json().get("response", "")
                    return self._parse_json(response_text)
                return None
        except Exception as e:
            print(f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨ ({model}): {e}")
            return None

    def _check_rate_limit(self) -> bool:
        """Gemini API í˜¸ì¶œ ì œí•œ ì²´í¬ (ë¶„ë‹¹ 14íšŒ). ì†Œì§„ ì‹œ False â†’ Ollama í´ë°±"""
        current_time = time.time()
        self.request_history = [t for t in self.request_history if current_time - t < 60]
        return len(self.request_history) < 14

    def _is_quota_exhausted(self, e: Exception) -> bool:
        """Gemini í• ë‹¹ëŸ‰/ì¿¼í„° ì†Œì§„ ì—ëŸ¬ ì—¬ë¶€ (429, 403 ë“±)"""
        msg = str(e).lower()
        return "429" in msg or "quota" in msg or "resource exhausted" in msg or "rate limit" in msg

    async def analyze_document(self, text: str, db: Optional[Session] = None, service_db: Optional[Session] = None, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ë¶„ì„ ë©”ì¸ íŒŒì´í”„ë¼ì¸ (RAG ê³ ë„í™” ë²„ì „)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            db: AI DB ì„¸ì…˜ (quantum_ai) - ìƒê´€ê´€ê³„ ê·œì¹™ ë“±
            service_db: Service DB ì„¸ì…˜ (quantum_service) - knowledge_base RAG ì¡°íšŒìš© (READ ONLY)
            options: ë¶„ì„ ì˜µì…˜ (main_category, sub_category, render_type ë“±)
        """
        # 1. ì¹´í…Œê³ ë¦¬ ê²°ì • (ì‚¬ìš©ì ì…ë ¥ ìš°ì„ , ì—†ìœ¼ë©´ ìë™ ê°ì§€)
        main_cat = options.get("main_category")
        sub_cat = options.get("sub_category")
        
        if main_cat and sub_cat:
            category = f"{main_cat}_{sub_cat}"
        else:
            category = detect_category(text)
            
        cat_info = CATEGORIES.get(category, CATEGORIES["GENERAL_DOC"])
        
        # 2. ê³ ë„í™”ëœ ì§€ì‹ ë² ì´ìŠ¤(RAG) ì¡°íšŒ - Service DBì—ì„œ ì½ê¸° ì „ìš©
        knowledge_context = ""
        knowledge_items = []
        if service_db:
            # knowledge_base í…Œì´ë¸”ì€ quantum_service DBì— ìœ„ì¹˜ (Java Admin WASê°€ ê´€ë¦¬)
            # ëª¨ë“  í‹°ì–´ì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í™•ì¥ (ê¸°ì¡´ PRO ì „ìš©ì—ì„œ ë³€ê²½)
            # ìµœì‹ ìˆœìœ¼ë¡œ ìƒìœ„ 5ê°œì˜ ê´€ë ¨ ì§€ì‹ ì¶”ì¶œ
            knowledge_items = service_db.query(KnowledgeBase).filter(
                KnowledgeBase.category.like(f"{main_cat}%") if main_cat else KnowledgeBase.category == category,
                KnowledgeBase.is_active == True
            ).order_by(KnowledgeBase.updated_at.desc()).limit(5).all()
            
            if knowledge_items:
                knowledge_context = "\n[ì¤‘ìš” ì§€ì‹ ë² ì´ìŠ¤ ë° ì‹œê°í™” ê·œì¹™]\n"
                for k in knowledge_items:
                    # ì§€ì‹ì˜ ì¶œì²˜ì™€ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ ì£¼ì…
                    knowledge_context += f"- [{k.title}]: {k.content} (ì¶œì²˜: {k.source_url or 'ë‚´ë¶€ ì§€ì‹'})\n"

        # 3. ë¶„ì„ ìˆ˜í–‰ (í‹°ì–´ë³„ ëª¨ë¸ ì„ íƒ)
        result = await self._call_ai_with_tier(text, category, knowledge_context, options)
        
        # ê²°ê³¼ì— ì¹´í…Œê³ ë¦¬ ì •ë³´ ë° RAG í™œìš© ì—¬ë¶€ ì¶”ê°€
        result["detected_category"] = category
        result["model_tier"] = cat_info.tier.value
        result["rag_applied"] = len(knowledge_items) > 0
        return result

    async def analyze_structured_table(
        self,
        parsed_table: List[Dict[str, Any]],
        db: Optional[Session] = None,
        service_db: Optional[Session] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        íŒŒì‹±ëœ í…Œì´ë¸” ë°ì´í„°ë¥¼ AIê°€ ë¶„ì„ (ìš”ì•½, í‚¤ì›Œë“œ, ê´€ê³„ ì¶”ì¶œ).
        ì‹œê°í™”ëŠ” íŒŒì„œ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ê³ , AIëŠ” ì¸ì‚¬ì´íŠ¸ ë³´ê°•ìš©.
        """
        # êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ (ìƒìœ„ 30í–‰ë§Œ, í† í° ì ˆì•½)
        sample = parsed_table[:30] if len(parsed_table) > 30 else parsed_table
        data_str = json.dumps(sample, ensure_ascii=False, indent=2)
        
        options = options or {}
        main_cat = options.get("main_category")
        sub_cat = options.get("sub_category")
        category = f"{main_cat}_{sub_cat}" if (main_cat and sub_cat) else "GENERAL_DOC"
        cat_info = CATEGORIES.get(category, CATEGORIES["GENERAL_DOC"])
        
        knowledge_context = ""
        if service_db and main_cat:
            knowledge_items = service_db.query(KnowledgeBase).filter(
                KnowledgeBase.category.like(f"{main_cat}%"),
                KnowledgeBase.is_active == True
            ).order_by(KnowledgeBase.updated_at.desc()).limit(3).all()
            if knowledge_items:
                knowledge_context = "\n[ì°¸ê³  ì§€ì‹]\n" + "\n".join(f"- {k.title}: {k.content[:200]}" for k in knowledge_items)
        
        prompt = f"""ë‹¤ìŒì€ ì´ë¯¸ êµ¬ì¡°í™”ëœ í…Œì´ë¸” ë°ì´í„°ì…ë‹ˆë‹¤. íŒŒì‹±ì€ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹] JSONë§Œ ì¶œë ¥:
{{
    "summary": "ë°ì´í„°ì˜ í•µì‹¬ ìš”ì•½ (í•œêµ­ì–´, 1~2ë¬¸ì¥)",
    "keywords": [{{"term": "í•µì‹¬ í‚¤ì›Œë“œ", "value": "ëŒ€í‘œê°’", "definition": "í•´ì„", "importance": 1-10}}],
    "relations": [{{"source": "í•­ëª©A", "target": "í•­ëª©B", "label": "ê´€ê³„", "strength": 1-10}}],
    "suggested_2d_viz": "table" ë˜ëŠ” "card" ë˜ëŠ” "chart" ë˜ëŠ” "network"
}}
suggested_2d_viz ê·œì¹™: ìˆ˜ì¹˜(ê¸ˆì•¡Â·ê¸°ì˜¨ ë“±) ë§ìœ¼ë©´ chart, ê´€ê³„/êµ¬ì¡°ê°€ ìˆìœ¼ë©´ network, ì¼ë°˜ í…ìŠ¤íŠ¸/íŒë¡€/ì†ŒìŠ¤ëŠ” table, í‚¤ì›Œë“œ ì¤‘ì‹¬ì´ë©´ card.

{knowledge_context}

[êµ¬ì¡°í™”ëœ í…Œì´ë¸” ë°ì´í„°]
{data_str[:3000]}
"""
        try:
            # 1. Gemini ìš°ì„ . í•œë„ ì†Œì§„ ì‹œ Ollama í´ë°±
            if not self.cloud_available:
                print("âš ï¸ Gemini API í‚¤ ë¯¸ì„¤ì • â†’ Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            elif not self._check_rate_limit():
                print("âš ï¸ Gemini í˜¸ì¶œ í•œë„ ì†Œì§„ (ë¶„ë‹¹ 14íšŒ) â†’ Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            if self.cloud_available and self._check_rate_limit():
                try:
                    self.request_history.append(time.time())
                    model_name = "gemini-2.5-flash"
                    response = self.client.models.generate_content(model=model_name, contents=prompt)
                    if response and response.text:
                        result = self._parse_json(response.text)
                        result["detected_category"] = category
                        result["model_tier"] = "flash"
                        result["suggested_render"] = "settlement"
                        return result
                except Exception as e:
                    if self._is_quota_exhausted(e):
                        print(f"âš ï¸ Gemini í˜¸ì¶œ í•œë„ ì†Œì§„ â†’ Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    else:
                        print(f"âš ï¸ Gemini ì‹¤íŒ¨: {e}. Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            # 2. Ollama í´ë°± (Gemini ì‹¤íŒ¨/ë¯¸ì„¤ì • ì‹œ)
            if cat_info.tier in [ModelTier.FLASH, ModelTier.PRO]:
                result = await self._call_ollama("llama3.2", prompt)
                if result and (result.get("summary") or result.get("keywords")):
                    result["detected_category"] = category
                    result["model_tier"] = cat_info.tier.value
                    result["suggested_render"] = "settlement"
                    return result
            # 3. TinyLlama ìµœí›„ ìˆ˜ë‹¨
            result = await self._run_local_model(prompt)
            result["detected_category"] = category
            result["model_tier"] = "local"
            result["suggested_render"] = "settlement"
            return result
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡°í™” í…Œì´ë¸” AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "summary": f"í…Œì´ë¸” ({len(parsed_table)}í–‰) â€” AI ë¶„ì„ ì—†ì´ ì‹œê°í™”",
                "keywords": [],
                "relations": [],
                "detected_category": category,
                "model_tier": "none",
                "suggested_render": "settlement",
                "suggested_2d_viz": "table"
            }

    async def _call_ai_with_tier(self, text: str, category: str, knowledge: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ í‹°ì–´ì— ë”°ë¥¸ ëª¨ë¸ í˜¸ì¶œ ë¶„ê¸°"""
        cat_info = CATEGORIES[category]
        options = options or {}
        render_type = options.get("render_type", "auto")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_specialized_prompt(text, category, knowledge, render_type)

        # 1. Gemini ìš°ì„  (API í‚¤ ìˆìœ¼ë©´). í•œë„ ì†Œì§„ ì‹œ Ollama í´ë°±
        if not self.cloud_available:
            print("âš ï¸ Gemini API í‚¤ ë¯¸ì„¤ì • â†’ Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        elif not self._check_rate_limit():
            print("âš ï¸ Gemini í˜¸ì¶œ í•œë„ ì†Œì§„ (ë¶„ë‹¹ 14íšŒ) â†’ Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        elif self.cloud_available and self._check_rate_limit():
            model_name = "gemini-2.5-pro" if cat_info.tier == ModelTier.PRO else "gemini-2.5-flash"
            print(f"ğŸš€ [Mode: Cloud] Requesting {model_name} for category {category}...")
            
            try:
                self.request_history.append(time.time())
                # Pro ëª¨ë¸ì¸ ê²½ìš° êµ¬ê¸€ ê²€ìƒ‰(Grounding) í™œìš©
                config = {"tools": [{"google_search": {}}]} if cat_info.tier == ModelTier.PRO else {}
                
                response = self.client.models.generate_content(
                    model=model_name,
                    contents=prompt,
                    config=config
                )
                if response and response.text:
                    return self._parse_json(response.text)
            except Exception as e:
                if self._is_quota_exhausted(e):
                    print(f"âš ï¸ Gemini í˜¸ì¶œ í•œë„ ì†Œì§„ â†’ Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                else:
                    print(f"âš ï¸ Cloud API ì‹¤íŒ¨: {e}. Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        
        # 2. Ollama í´ë°± (Gemini ì‹¤íŒ¨/ë¯¸ì„¤ì • ì‹œ)
        if cat_info.tier in [ModelTier.FLASH, ModelTier.PRO]:
            ollama_result = await self._call_ollama("llama3.2", prompt)
            if ollama_result and ollama_result.get("keywords"):
                return ollama_result
        
        # 3. ìµœí›„ì˜ ìˆ˜ë‹¨: TinyLlama (Transformers)
        return await self._run_local_model(prompt)

    def _build_specialized_prompt(self, text: str, category: str, knowledge: str, render_type: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± (RAG ê°•í™” ë²„ì „)"""
        cat_info = CATEGORIES[category]
        use_settlement = render_type == "settlement" or any(k in text for k in ["ë§Œì›", "ì›", "ê¸ˆì•¡", "ë§¤ì¶œ", "ë§¤ì…", "ë¹„ìš©"])
        suggested_render = "settlement" if use_settlement else render_type

        table_data_instruction = ""
        if use_settlement:
            table_data_instruction = """
        [ì°¨íŠ¸/ì •ì‚°ìš©] ìˆ«ì(ê¸ˆì•¡Â·ë¹„ìœ¨)ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ "table_data" ë°°ì—´ì„ í¬í•¨í•˜ì„¸ìš”.
        í˜•ì‹: [{"í•­ëª©":"ì´ë¦„","ê¸ˆì•¡":ìˆ«ì},{"í•­ëª©":"ì´ë¦„2","ê¸ˆì•¡":ìˆ«ì2},...]
        ì˜ˆ: "ë§¤ì¶œ 12500ë§Œì›, ë§¤ì… 7200" â†’ "table_data":[{"í•­ëª©":"ë§¤ì¶œ","ê¸ˆì•¡":12500},{"í•­ëª©":"ë§¤ì…","ê¸ˆì•¡":7200}]
        """

        return f"""
        ë‹¹ì‹ ì€ {cat_info.description} ë¶„ì•¼ì˜ ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        [ë°ì´í„°]ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.{table_data_instruction}

        [ì§€ì‹ ë² ì´ìŠ¤] {knowledge if knowledge else "í•´ë‹¹ ì—†ìŒ."}

        [ì¶œë ¥ í˜•ì‹] JSONë§Œ ì¶œë ¥:
        {{
            "summary": "í•µì‹¬ ìš”ì•½ (í•œêµ­ì–´)",
            "suggested_render": "{suggested_render}",
            "suggested_2d_viz": "table" ë˜ëŠ” "card" ë˜ëŠ” "chart" ë˜ëŠ” "network",
            "table_data": [{{"í•­ëª©": "í•­ëª©ëª…", "ê¸ˆì•¡": ìˆ«ì}}],
            "keywords": [{{"term": "í‚¤ì›Œë“œ", "value": "ê°’", "definition": "í•´ì„", "importance": 1-10}}],
            "relations": [{{"source": "A", "target": "B", "label": "ê´€ê³„", "strength": 1-10}}]
        }}
        suggested_2d_viz ê·œì¹™: ìˆ˜ì¹˜(ê¸ˆì•¡Â·ê¸°ì˜¨) ë§ìœ¼ë©´ chart, ê´€ê³„/ì˜ì¡´ì„± ìˆìœ¼ë©´ network, íŒë¡€Â·ì†ŒìŠ¤ì½”ë“œÂ·í…ìŠ¤íŠ¸ëŠ” table, í‚¤ì›Œë“œ ì¤‘ì‹¬ì´ë©´ card.

        [ë°ì´í„°]
        {text[:2000]}
        """

    async def _run_local_model(self, prompt: str) -> Dict[str, Any]:
        """ë¡œì»¬ AI ëª¨ë¸(TinyLlama) ì‹¤í–‰"""
        print("ğŸ  [Mode: Local] Running Internal Model...")
        try:
            if self.local_pipeline is None:
                self.local_pipeline = pipeline(
                    "text-generation", 
                    model=self.local_model_id, 
                    device_map="auto" if torch.cuda.is_available() else None,
                    torch_dtype=torch.float32
                )
            
            outputs = self.local_pipeline(prompt, max_new_tokens=512, do_sample=True, temperature=0.7)
            return self._parse_json(outputs[0]["generated_text"])
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"summary": "ë¶„ì„ ì‹¤íŒ¨", "keywords": [], "relations": []}

    def _parse_json(self, text: str) -> Dict[str, Any]:
        """AI ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
        try:
            json_str = re.search(r'\{.*\}', text, re.DOTALL).group()
            return json.loads(json_str)
        except:
            return {"summary": "ë°ì´í„° íŒŒì‹± ì—ëŸ¬", "keywords": [], "relations": []}

ai_agent_service = AIAgentService()
