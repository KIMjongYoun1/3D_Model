"""
AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ (ì•ˆì •í™” ë²„ì „)
"""
import time
import json
import torch
import re
from typing import Dict, Any, List, Optional
from google import genai
from transformers import pipeline
from app.core.config import settings
from app.core.document_processor import document_processor

class AIAgentService:
    def __init__(self):
        self.cloud_available = False
        try:
            if settings.gemini_api_key:
                # API í‚¤ê°€ ìˆìœ¼ë©´ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                self.client = genai.Client(api_key=settings.gemini_api_key)
                self.cloud_available = True
            else:
                print("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Gemini ì´ˆê¸°í™” ì—ëŸ¬: {e}")
        
        self.local_pipeline = None
        self.local_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        self.request_history = []

    def _check_rate_limit(self) -> bool:
        current_time = time.time()
        self.request_history = [t for t in self.request_history if current_time - t < 60]
        return len(self.request_history) < 14

    async def analyze_document(self, text: str, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ë¬¸ì„œ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
        chunks = document_processor.split_text(text)
        
        # ì²« ë²ˆì§¸ ì²­í¬ë§Œ ë¶„ì„í•˜ì—¬ ì†ë„ì™€ ì•ˆì •ì„± í™•ë³´
        chunk = chunks[0] if chunks else text
        result = await self._call_ai(chunk, options)
        return result

    async def _call_ai(self, text: str, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        options = options or {}
        render_type = options.get("render_type", "auto")
        
        render_instruction = ""
        if render_type == "settlement":
            render_instruction = "ì´ ë°ì´í„°ëŠ” 'ì •ì‚°/í†µê³„' ëª©ì ì…ë‹ˆë‹¤. ìˆ˜ì¹˜ ë°ì´í„° ìœ„ì£¼ë¡œ 'table_data' í•„ë“œì— ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”."
        elif render_type == "diagram":
            render_instruction = "ì´ ë°ì´í„°ëŠ” 'ê´€ê³„ë„/êµ¬ì¡°' ëª©ì ì…ë‹ˆë‹¤. í‚¤ì›Œë“œ ê°„ì˜ ì—°ê²° ê´€ê³„ ìœ„ì£¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”."

        prompt = f"""
        ë‹¹ì‹ ì€ ë°ì´í„°ì˜ í•µì‹¬ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³  ì‹¤ì‹œê°„ ê²€ìƒ‰ì„ í†µí•´ ì „ë¬¸ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ëŠ” 3D ì§€ì‹ ë§µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , í•„ìš”í•œ ê²½ìš° êµ¬ê¸€ ê²€ìƒ‰ì„ í†µí•´ ê¸°ìˆ  ìš©ì–´, ë²•ì¡°í•­, ìµœì‹  ì‚¬ë¡€ ë“±ì˜ ê·¼ê±°ë¥¼ ì°¾ì•„ ë‹¤ìŒ JSON êµ¬ì¡°ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

        {{
            "summary": "ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ 5ì¤„ í•µì‹¬ ìš”ì•½ (í•œêµ­ì–´)",
            "suggested_render": "{render_type}",
            "keywords": [
                {{
                    "term": "í•µì‹¬ í‚¤ì›Œë“œ",
                    "value": "ì‹¤ì œ ê°’/ë¬¸ì¥",
                    "definition": "ê¸°ëŠ¥ ì„¤ëª…/í•´ì„",
                    "importance": 1-10,
                    "references": [
                        {{"title": "ì¶œì²˜ ëª…ì¹­ (ì˜ˆ: ë²•ë ¹ëª…, ê¸°ìˆ ë¬¸ì„œ ì œëª©)", "url": "ì›ë¬¸ ë§í¬", "snippet": "ì°¸ê³ í•œ í•µì‹¬ ë¬¸êµ¬ ìš”ì•½"}}
                    ]
                }}
            ],
            "table_data": [
                {{"í•­ëª©": "ê°’", "ê¸ˆì•¡": 1000, "ë¹„ê³ ": "..."}}
            ],
            "relations": [
                {{"source": "í‚¤ì›Œë“œA", "target": "í‚¤ì›Œë“œB", "label": "ê´€ê³„ ì„¤ëª…", "strength": 1-10}}
            ]
        }}

        [ì¤‘ìš” ì§€ì¹¨]
        1. ì „ë¬¸ ìš©ì–´ë‚˜ ë²•ì  ê·¼ê±°ê°€ í•„ìš”í•œ ê²½ìš° ë°˜ë“œì‹œ ì‹¤ì‹œê°„ ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ 'references'ë¥¼ ì±„ìš°ì„¸ìš”.
        2. ì €ì‘ê¶Œì„ ì¡´ì¤‘í•˜ì—¬ ì°¸ê³ í•œ ë¬¸ì„œì˜ ì œëª©ê³¼ ì •í™•í•œ URLì„ ì œê³µí•˜ì„¸ìš”.
        3. ëª¨ë“  ì„¤ëª…ê³¼ í•´ì„ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        4. ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

        ë°ì´í„°:
        {text}
        """

        if self.cloud_available and self._check_rate_limit():
            print("ğŸš€ [Mode: Cloud] Requesting Gemini with Google Search...")
            try:
                self.request_history.append(time.time())
                # êµ¬ê¸€ ê²€ìƒ‰(Grounding) ë„êµ¬ í™œì„±í™”
                response = self.client.models.generate_content(
                    model="gemini-flash-latest",
                    contents=prompt,
                    config={
                        "tools": [{"google_search": {}}]
                    }
                )
                if response and response.text:
                    return self._parse_json(response.text)
            except Exception as e:
                print(f"âš ï¸ Cloud API ì‹¤íŒ¨: {e}. ë¡œì»¬ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        
        return await self._run_local_model(prompt)

    async def _run_local_model(self, prompt: str) -> Dict[str, Any]:
        print("ğŸ  [Mode: Local] Running Internal Model...")
        try:
            if self.local_pipeline is None:
                # CPU í™˜ê²½ì—ì„œë„ ëŒì•„ê°€ë„ë¡ ìµœì í™” ì„¤ì •
                self.local_pipeline = pipeline(
                    "text-generation", 
                    model=self.local_model_id, 
                    device_map="auto" if torch.cuda.is_available() else None,
                    torch_dtype=torch.float32 # ë§¥/ìœˆë„ìš° í˜¸í™˜ì„±ì„ ìœ„í•´ float32 ì‚¬ìš©
                )
            
            outputs = self.local_pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)
            return self._parse_json(outputs[0]["generated_text"])
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"summary": "ë¶„ì„ ì‹¤íŒ¨", "keywords": ["Error"], "relations": []}

    def _parse_json(self, text: str) -> Dict[str, Any]:
        try:
            json_str = re.search(r'\{.*\}', text, re.DOTALL).group()
            return json.loads(json_str)
        except:
            return {"summary": "ë°ì´í„° íŒŒì‹± ì—ëŸ¬", "keywords": ["Parse Error"], "relations": []}

ai_agent_service = AIAgentService()
