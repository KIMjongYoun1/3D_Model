"""
AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ (ì•ˆì •í™” ë²„ì „)
"""
import time
import json
import torch
import re
from typing import Dict, Any, List
from google import genai
from transformers import pipeline
from app.core.config import settings
from app.core.document_processor import document_processor

class AIAgentService:
    def __init__(self):
        self.cloud_available = False
        try:
            if settings.gemini_api_key:
                # API í‚¤ê°€ ìˆìœ¼ë©´ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                self.client = genai.Client(api_key=settings.gemini_api_key)
                self.cloud_available = True
            else:
                print("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Gemini ì´ˆê¸°í™” ì—ëŸ¬: {e}")
        
        self.local_pipeline = None
        self.local_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        self.request_history = []

    def _check_rate_limit(self) -> bool:
        current_time = time.time()
        self.request_history = [t for t in self.request_history if current_time - t < 60]
        return len(self.request_history) < 14

    async def analyze_document(self, text: str) -> Dict[str, Any]:
        """ë¬¸ì„œ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
        chunks = document_processor.split_text(text)
        all_results = []
        
        # ì²« ë²ˆì§¸ ì²­í¬ë§Œ ë¶„ì„í•˜ì—¬ ì†ë„ì™€ ì•ˆì •ì„± í™•ë³´
        chunk = chunks[0] if chunks else text
        result = await self._call_ai(chunk)
        return result

    async def _call_ai(self, text: str) -> Dict[str, Any]:
        prompt = f"""
        Analyze the following text and provide a JSON response with this structure:
        {{
            "summary": "5-line summary in Korean",
            "keywords": ["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"],
            "relations": [
                {{"source": "keywordA", "target": "keywordB", "label": "description"}}
            ]
        }}
        Text: {text}
        """

        if self.cloud_available and self._check_rate_limit():
            print("ğŸš€ [Mode: Cloud] Requesting Gemini...")
            try:
                self.request_history.append(time.time())
                # ëª¨ë¸ ì´ë¦„ì„ 'gemini-flash-latest'ë¡œ ìˆ˜ì • (í˜¸í™˜ì„± í™•ì¸ë¨)
                response = self.client.models.generate_content(
                    model="gemini-flash-latest",
                    contents=prompt
                )
                if response and response.text:
                    return self._parse_json(response.text)
            except Exception as e:
                print(f"âš ï¸ Cloud API ì‹¤íŒ¨: {e}. ë¡œì»¬ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        
        return await self._run_local_model(prompt)

    async def _run_local_model(self, prompt: str) -> Dict[str, Any]:
        print("ğŸ  [Mode: Local] Running Internal Model...")
        try:
            if self.local_pipeline is None:
                # CPU í™˜ê²½ì—ì„œë„ ëŒì•„ê°€ë„ë¡ ìµœì í™” ì„¤ì •
                self.local_pipeline = pipeline(
                    "text-generation", 
                    model=self.local_model_id, 
                    device_map="auto" if torch.cuda.is_available() else None,
                    torch_dtype=torch.float32 # ë§¥/ìœˆë„ìš° í˜¸í™˜ì„±ì„ ìœ„í•´ float32 ì‚¬ìš©
                )
            
            outputs = self.local_pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)
            return self._parse_json(outputs[0]["generated_text"])
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"summary": "ë¶„ì„ ì‹¤íŒ¨", "keywords": ["Error"], "relations": []}

    def _parse_json(self, text: str) -> Dict[str, Any]:
        try:
            json_str = re.search(r'\{.*\}', text, re.DOTALL).group()
            return json.loads(json_str)
        except:
            return {"summary": "ë°ì´í„° íŒŒì‹± ì—ëŸ¬", "keywords": ["Parse Error"], "relations": []}

ai_agent_service = AIAgentService()
