"""
AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ (ê³ ë„í™” ë²„ì „)
- ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ í‹°ì–´ë§ (Local, Flash, Pro)
- ì§€ì‹ ë² ì´ìŠ¤(RAG) ì—°ë™
- í•œêµ­ì–´ ì£¼ì„ ë° ì„¤ëª… í¬í•¨
"""
import time
import json
import torch
import re
import httpx
from typing import Dict, Any, List, Optional
from google import genai
from transformers import pipeline
from sqlalchemy.orm import Session

from app.core.config import settings
from app.core.document_processor import document_processor
from app.core.categories import detect_category, CATEGORIES, ModelTier
from app.models.knowledge import KnowledgeBase

class AIAgentService:
    def __init__(self):
        self.cloud_available = False
        try:
            if settings.gemini_api_key:
                self.client = genai.Client(api_key=settings.gemini_api_key)
                self.cloud_available = True
            else:
                print("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Gemini ì´ˆê¸°í™” ì—ëŸ¬: {e}")
        
        self.local_pipeline = None
        self.local_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        self.ollama_base_url = "http://localhost:11434"
        self.request_history = []

    async def _call_ollama(self, model: str, prompt: str) -> Optional[Dict[str, Any]]:
        """Ollama APIë¥¼ í†µí•œ ë¡œì»¬ ëª¨ë¸(Phi-4 ë“±) í˜¸ì¶œ"""
        print(f"ğŸ¦™ [Mode: Ollama] Requesting {model}...")
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.ollama_base_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.2,
                            "top_p": 0.9
                        }
                    }
                )
                if response.status_code == 200:
                    response_text = response.json().get("response", "")
                    return self._parse_json(response_text)
                return None
        except Exception as e:
            print(f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨ ({model}): {e}")
            return None

    def _check_rate_limit(self) -> bool:
        """API í˜¸ì¶œ ì œí•œ ì²´í¬ (ë¶„ë‹¹ 14íšŒ ì œí•œ)"""
        current_time = time.time()
        self.request_history = [t for t in self.request_history if current_time - t < 60]
        return len(self.request_history) < 14

    async def analyze_document(self, text: str, db: Optional[Session] = None, service_db: Optional[Session] = None, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ë¶„ì„ ë©”ì¸ íŒŒì´í”„ë¼ì¸ (RAG ê³ ë„í™” ë²„ì „)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            db: AI DB ì„¸ì…˜ (quantum_ai) - ìƒê´€ê´€ê³„ ê·œì¹™ ë“±
            service_db: Service DB ì„¸ì…˜ (quantum_service) - knowledge_base RAG ì¡°íšŒìš© (READ ONLY)
            options: ë¶„ì„ ì˜µì…˜ (main_category, sub_category, render_type ë“±)
        """
        # 1. ì¹´í…Œê³ ë¦¬ ê²°ì • (ì‚¬ìš©ì ì…ë ¥ ìš°ì„ , ì—†ìœ¼ë©´ ìë™ ê°ì§€)
        main_cat = options.get("main_category")
        sub_cat = options.get("sub_category")
        
        if main_cat and sub_cat:
            category = f"{main_cat}_{sub_cat}"
        else:
            category = detect_category(text)
            
        cat_info = CATEGORIES.get(category, CATEGORIES["GENERAL_DOC"])
        
        # 2. ê³ ë„í™”ëœ ì§€ì‹ ë² ì´ìŠ¤(RAG) ì¡°íšŒ - Service DBì—ì„œ ì½ê¸° ì „ìš©
        knowledge_context = ""
        knowledge_items = []
        if service_db:
            # knowledge_base í…Œì´ë¸”ì€ quantum_service DBì— ìœ„ì¹˜ (Java Admin WASê°€ ê´€ë¦¬)
            # ëª¨ë“  í‹°ì–´ì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í™•ì¥ (ê¸°ì¡´ PRO ì „ìš©ì—ì„œ ë³€ê²½)
            # ìµœì‹ ìˆœìœ¼ë¡œ ìƒìœ„ 5ê°œì˜ ê´€ë ¨ ì§€ì‹ ì¶”ì¶œ
            knowledge_items = service_db.query(KnowledgeBase).filter(
                KnowledgeBase.category.like(f"{main_cat}%") if main_cat else KnowledgeBase.category == category,
                KnowledgeBase.is_active == True
            ).order_by(KnowledgeBase.updated_at.desc()).limit(5).all()
            
            if knowledge_items:
                knowledge_context = "\n[ì¤‘ìš” ì§€ì‹ ë² ì´ìŠ¤ ë° ì‹œê°í™” ê·œì¹™]\n"
                for k in knowledge_items:
                    # ì§€ì‹ì˜ ì¶œì²˜ì™€ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ ì£¼ì…
                    knowledge_context += f"- [{k.title}]: {k.content} (ì¶œì²˜: {k.source_url or 'ë‚´ë¶€ ì§€ì‹'})\n"

        # 3. ë¶„ì„ ìˆ˜í–‰ (í‹°ì–´ë³„ ëª¨ë¸ ì„ íƒ)
        result = await self._call_ai_with_tier(text, category, knowledge_context, options)
        
        # ê²°ê³¼ì— ì¹´í…Œê³ ë¦¬ ì •ë³´ ë° RAG í™œìš© ì—¬ë¶€ ì¶”ê°€
        result["detected_category"] = category
        result["model_tier"] = cat_info.tier.value
        result["rag_applied"] = len(knowledge_items) > 0
        return result

    async def _call_ai_with_tier(self, text: str, category: str, knowledge: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ í‹°ì–´ì— ë”°ë¥¸ ëª¨ë¸ í˜¸ì¶œ ë¶„ê¸°"""
        cat_info = CATEGORIES[category]
        options = options or {}
        render_type = options.get("render_type", "auto")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_specialized_prompt(text, category, knowledge, render_type)

        # 1. ë³´ì•ˆ ë° ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ê³ ì„±ëŠ¥ ë¡œì»¬ ëª¨ë¸(Llama 3.2) ìš°ì„  ì‹œë„ (Ollama)
        if cat_info.tier in [ModelTier.FLASH, ModelTier.PRO]:
            ollama_result = await self._call_ollama("llama3.2", prompt)
            if ollama_result and ollama_result.get("keywords"):
                return ollama_result

        # 2. Cloud (Gemini) - Ollama ì‹¤íŒ¨ ì‹œ ë˜ëŠ” íŠ¹ì • í‹°ì–´ì—ì„œ ì‚¬ìš©
        if self.cloud_available and self._check_rate_limit():
            model_name = "gemini-1.5-pro-latest" if cat_info.tier == ModelTier.PRO else "gemini-1.5-flash-latest"
            print(f"ğŸš€ [Mode: Cloud] Requesting {model_name} for category {category}...")
            
            try:
                self.request_history.append(time.time())
                # Pro ëª¨ë¸ì¸ ê²½ìš° êµ¬ê¸€ ê²€ìƒ‰(Grounding) í™œìš©
                config = {"tools": [{"google_search": {}}]} if cat_info.tier == ModelTier.PRO else {}
                
                response = self.client.models.generate_content(
                    model=model_name,
                    contents=prompt,
                    config=config
                )
                if response and response.text:
                    return self._parse_json(response.text)
            except Exception as e:
                print(f"âš ï¸ Cloud API ì‹¤íŒ¨: {e}. ë¡œì»¬ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        
        # 3. ìµœí›„ì˜ ìˆ˜ë‹¨: ì´ˆê²½ëŸ‰ ë¡œì»¬ ëª¨ë¸ (TinyLlama - Transformers)
        return await self._run_local_model(prompt)

    def _build_specialized_prompt(self, text: str, category: str, knowledge: str, render_type: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± (RAG ê°•í™” ë²„ì „)"""
        cat_info = CATEGORIES[category]
        use_settlement = render_type == "settlement" or any(k in text for k in ["ë§Œì›", "ì›", "ê¸ˆì•¡", "ë§¤ì¶œ", "ë§¤ì…", "ë¹„ìš©"])
        suggested_render = "settlement" if use_settlement else render_type

        table_data_instruction = ""
        if use_settlement:
            table_data_instruction = """
        [ì°¨íŠ¸/ì •ì‚°ìš©] ìˆ«ì(ê¸ˆì•¡Â·ë¹„ìœ¨)ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ "table_data" ë°°ì—´ì„ í¬í•¨í•˜ì„¸ìš”.
        í˜•ì‹: [{"í•­ëª©":"ì´ë¦„","ê¸ˆì•¡":ìˆ«ì},{"í•­ëª©":"ì´ë¦„2","ê¸ˆì•¡":ìˆ«ì2},...]
        ì˜ˆ: "ë§¤ì¶œ 12500ë§Œì›, ë§¤ì… 7200" â†’ "table_data":[{"í•­ëª©":"ë§¤ì¶œ","ê¸ˆì•¡":12500},{"í•­ëª©":"ë§¤ì…","ê¸ˆì•¡":7200}]
        """

        return f"""
        ë‹¹ì‹ ì€ {cat_info.description} ë¶„ì•¼ì˜ ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        [ë°ì´í„°]ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.{table_data_instruction}

        [ì§€ì‹ ë² ì´ìŠ¤] {knowledge if knowledge else "í•´ë‹¹ ì—†ìŒ."}

        [ì¶œë ¥ í˜•ì‹] JSONë§Œ ì¶œë ¥:
        {{
            "summary": "í•µì‹¬ ìš”ì•½ (í•œêµ­ì–´)",
            "suggested_render": "{suggested_render}",
            "table_data": [{{"í•­ëª©": "í•­ëª©ëª…", "ê¸ˆì•¡": ìˆ«ì}}],
            "keywords": [{{"term": "í‚¤ì›Œë“œ", "value": "ê°’", "definition": "í•´ì„", "importance": 1-10}}],
            "relations": [{{"source": "A", "target": "B", "label": "ê´€ê³„", "strength": 1-10}}]
        }}

        [ë°ì´í„°]
        {text[:2000]}
        """

    async def _run_local_model(self, prompt: str) -> Dict[str, Any]:
        """ë¡œì»¬ AI ëª¨ë¸(TinyLlama) ì‹¤í–‰"""
        print("ğŸ  [Mode: Local] Running Internal Model...")
        try:
            if self.local_pipeline is None:
                self.local_pipeline = pipeline(
                    "text-generation", 
                    model=self.local_model_id, 
                    device_map="auto" if torch.cuda.is_available() else None,
                    torch_dtype=torch.float32
                )
            
            outputs = self.local_pipeline(prompt, max_new_tokens=512, do_sample=True, temperature=0.7)
            return self._parse_json(outputs[0]["generated_text"])
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"summary": "ë¶„ì„ ì‹¤íŒ¨", "keywords": [], "relations": []}

    def _parse_json(self, text: str) -> Dict[str, Any]:
        """AI ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
        try:
            json_str = re.search(r'\{.*\}', text, re.DOTALL).group()
            return json.loads(json_str)
        except:
            return {"summary": "ë°ì´í„° íŒŒì‹± ì—ëŸ¬", "keywords": [], "relations": []}

ai_agent_service = AIAgentService()
